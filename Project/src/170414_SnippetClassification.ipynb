{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "164\n",
      "164\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "import string\n",
    "from os import listdir\n",
    "import numpy as np\n",
    "\n",
    "#Folder containing the labeled snippet subfolders for training the classifier\n",
    "snippetFolder = \"/home/abc/ADM/Train_Data\"\n",
    "\n",
    "#folder containing unlabeled snippets\n",
    "testSnippetFolder = \"/home/abc/ADM/Test_Data\"\n",
    "\n",
    "folder_list = [f for f in listdir(snippetFolder)]\n",
    "\n",
    "puncs = string.punctuation\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "for folder in folder_list:\n",
    "    files_list = [f for f in listdir(snippetFolder + \"/\" + folder)]\n",
    "    for file in files_list:\n",
    "        filePath = snippetFolder + \"/\" + folder + \"/\" + file\n",
    "        f = open(filePath,'r',encoding='utf-8')\n",
    "        data = f.readlines()\n",
    "        textSnippet = ''.join(data)\n",
    "        textSnippet = textSnippet.replace(\"\\n\",\"\")\n",
    "        for p in puncs:\n",
    "            if p in textSnippet:\n",
    "                textSnippet = textSnippet.replace(p,\"\")\n",
    "        X.append(textSnippet)\n",
    "        Y.append(folder)\n",
    "    \n",
    "print(len(X))\n",
    "print(len(Y))\n",
    "\n",
    "#from sklearn.feature_extraction import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#tfidf = TfidfVectorizer(input='content',encoding='utf-8',decode_error='ignore',strip_accents='ascii',ngram_range=(2,2),stop_words='english',max_df=80,min_df=6,max_features=None,norm='l1')\n",
    "tfidf = TfidfVectorizer(input='content',encoding='utf-8',decode_error='ignore',strip_accents='ascii',ngram_range=(2,2),stop_words='english',max_features=60,norm='l1')\n",
    "train = tfidf.fit_transform(X,Y)\n",
    "#print(tfidf.get_feature_names())\n",
    "#print(\"\\nWords Removed : \\n\",tfidf.stop_words_)\n",
    "#print(tfidf.build_analyzer())\n",
    "\n",
    "cv=5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.54285714  0.51515152  0.63636364  0.5625      0.61290323]\n",
      "0.573955104036\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Logistic Regression Classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf_lr = LogisticRegression()\n",
    "acc = cross_val_score(clf_lr,train,Y,scoring=\"accuracy\",cv=cv)\n",
    "print(acc)\n",
    "print(np.mean(acc))\n",
    "\n",
    "clf_lr.fit(train,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.34285714  0.36363636  0.36363636  0.34375     0.35483871]\n",
      "0.353743715961\n",
      "[ 0.51428571  0.51515152  0.63636364  0.53125     0.58064516]\n",
      "0.555539205418\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=0.1, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=123, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Support Vector Machine Classifier\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "\n",
    "clf_svc = SVC(C=0.1,random_state=123)\n",
    "acc = cross_val_score(clf_svc,train,Y,scoring=\"accuracy\",cv=cv)\n",
    "print(acc)\n",
    "print(np.mean(acc))\n",
    "\n",
    "clf_svc.fit(train,Y)\n",
    "\n",
    "clf_linear_svc = LinearSVC(C=0.1,random_state=123)\n",
    "acc = cross_val_score(clf_linear_svc,train,Y,scoring=\"accuracy\",cv=cv)\n",
    "print(acc)\n",
    "print(np.mean(acc))\n",
    "\n",
    "clf_linear_svc.fit(train,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Random Forest Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.74285714  0.66666667  0.63636364  0.6875      0.67741935]\n",
      "0.682161360145\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', alpha=0.01, batch_size='auto',\n",
       "       beta_1=0.9, beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(5, 10), learning_rate='adaptive',\n",
       "       learning_rate_init=0.001, max_iter=100, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=123,\n",
       "       shuffle=True, solver='lbfgs', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Neural Network Classifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "clf_NN = MLPClassifier(hidden_layer_sizes=(5,10),activation=\"logistic\",solver=\"lbfgs\",alpha=0.01,learning_rate=\"adaptive\",max_iter=100,shuffle=True,early_stopping=True,random_state=123)\n",
    "acc = cross_val_score(clf_NN,train,Y,scoring=\"accuracy\",cv=cv)\n",
    "print(acc)\n",
    "print(np.mean(acc))\n",
    "\n",
    "clf_NN.fit(train,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Clustering Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testData = []\n",
    "file_list = [f for f in listdir(testSnippetFolder)]\n",
    "\n",
    "for file in file_list:\n",
    "    filePath = testSnippetFolder + \"/\" + file\n",
    "    f = open(filePath,'r',encoding='utf-8')\n",
    "    data = f.readlines()\n",
    "    textSnippet = ''.join(data)\n",
    "    textSnippet = textSnippet.replace(\"\\n\",\"\")\n",
    "    for p in puncs:\n",
    "        if p in textSnippet:\n",
    "            textSnippet = textSnippet.replace(p,\"\")\n",
    "\n",
    "    testData.append(textSnippet)\n",
    "    \n",
    "#tfidf_test = TfidfVectorizer(input='content',encoding='utf-8',decode_error='ignore',strip_accents='ascii',ngram_range=(2,2),stop_words='english',max_features=None,norm='l1')\n",
    "test = tfidf.transform(testData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "35\n",
      "[('3102088__1.txt', 'CS_Sullivan'), ('hvd.32044039365119__10.txt', 'CS_Armour'), ('468039__33.txt', 'CS_Tallmadge'), ('nnc1.cu04264290__1.txt', 'CS_Armour'), ('1425278__2.txt', 'CS_Tallmadge'), ('mdp.39015031294864__1.txt', 'CS_Tallmadge'), ('774830__1.txt', 'CS_Tallmadge'), ('hvd.32044039365119__2.txt', 'CS_civics'), ('njp.32101059234532__1.txt', 'CS_Armour'), ('1997_1047944__2.txt', 'CS_Sullivan'), ('25505243__2.txt', 'CS_Armour'), ('nnc1.ar53666712__1.txt', 'CS_Sullivan'), ('nyp.33433071108827__4.txt', 'CS_Tallmadge'), ('1991_990613__2.txt', 'CS_Sullivan'), ('1959_1424016__1.txt', 'CS_Tallmadge'), ('773093__1.txt', 'CS_Armour'), ('mdp.39015082311138__1.txt', 'CS_Tallmadge'), ('hvd.32044039365119__9.txt', 'CS_Armour'), ('1424869__6.txt', 'CS_Tallmadge'), ('mdp.39015059791270__1.txt', 'CS_Tallmadge'), ('4117456__1.txt', 'CS_Armour'), ('ncs1.ark+=13960=t45q5jt4w__1.txt', 'CS_Tallmadge'), ('nyp.33433060463365__1.txt', 'CS_Tallmadge'), ('4102638__1.txt', 'CS_Armour'), ('mdp.39015046781350__1.txt', 'CS_Armour'), ('mdp.39015081896352__1.txt', 'CPS'), ('njp.32101076188927__1.txt', 'CS_Armour'), ('mdp.39015062797538__1.txt', 'CS_Sullivan'), ('njp.32101082377712__5.txt', 'CS_Armour'), ('hvd.32044039365119__3.txt', 'CS_civics'), ('njp.32101082377183__2.txt', 'CS_Tallmadge'), ('njp.32101082377423__1.txt', 'CS_Tallmadge'), ('1076072__1.txt', 'CPS'), ('mdp.39015062810638__1.txt', 'CS_Armour'), ('hvd.fl4rb6__1.txt', 'CS_Tallmadge')]\n"
     ]
    }
   ],
   "source": [
    "#print(clf_lr.coef_)\n",
    "\n",
    "Y_test_lr = clf_lr.predict(test)\n",
    "\n",
    "Y_test_svc = clf_svc.predict(test)\n",
    "\n",
    "Y_test_linear_svc = clf_linear_svc.predict(test)\n",
    "\n",
    "Y_test_nn = clf_NN.predict(test)\n",
    "\n",
    "print(len(file_list))\n",
    "print(len(Y_test_lr))\n",
    "\n",
    "l = len(file_list)\n",
    "final_list = []\n",
    "for i in range(l):\n",
    "    final_list.append((file_list[i],Y_test_nn[i]))\n",
    "print(final_list)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#find out the top features used by each classifier"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
