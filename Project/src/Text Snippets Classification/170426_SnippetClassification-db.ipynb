{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1450\n",
      "1450\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "import string\n",
    "from os import listdir\n",
    "import numpy as np\n",
    "\n",
    "#Folder containing the labeled snippet subfolders for training the classifier\n",
    "snippetFolder = \"Train_Data\"\n",
    "\n",
    "#folder containing unlabeled snippets\n",
    "testSnippetFolder = \"Test_Data\"\n",
    "\n",
    "folderToSaveResultsFile = \"Results\"\n",
    "\n",
    "folder_list = [f for f in listdir(snippetFolder) if not f.startswith('.')]\n",
    "\n",
    "puncs = string.punctuation\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "for folder in folder_list:\n",
    "    files_list = [f for f in listdir(snippetFolder + \"/\" + folder) if not f.startswith('.')]\n",
    "    for file in files_list:\n",
    "        filePath = snippetFolder + \"/\" + folder + \"/\" + file\n",
    "        f = open(filePath,'r',encoding='utf-8')\n",
    "        data = f.readlines()\n",
    "        textSnippet = ''.join(data)\n",
    "        textSnippet = textSnippet.replace(\"\\n\",\"\")\n",
    "        for p in puncs:\n",
    "            if p in textSnippet:\n",
    "                textSnippet = textSnippet.replace(p,\"\")\n",
    "        X.append(textSnippet)\n",
    "        Y.append(folder)\n",
    "    \n",
    "print(len(X))\n",
    "print(len(Y))\n",
    "\n",
    "#from sklearn.feature_extraction import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#tfidf = TfidfVectorizer(input='content',encoding='utf-8',decode_error='ignore',strip_accents='ascii',ngram_range=(2,2),stop_words='english',max_df=80,min_df=6,max_features=None,norm='l1')\n",
    "tfidf = TfidfVectorizer(input='content',encoding='utf-8',decode_error='ignore',strip_accents='ascii',ngram_range=(2,2),stop_words='english',max_features=60,norm='l1')\n",
    "train = tfidf.fit_transform(X,Y)\n",
    "#print(tfidf.get_feature_names())\n",
    "#print(\"\\nWords Removed : \\n\",tfidf.stop_words_)\n",
    "#print(tfidf.build_analyzer())\n",
    "\n",
    "cv=5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.83848797  0.86551724  0.86206897  0.84827586  0.8200692 ]\n",
      "0.846883849125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Logistic Regression Classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf_lr = LogisticRegression()\n",
    "acc = cross_val_score(clf_lr,train,Y,scoring=\"accuracy\",cv=cv)\n",
    "print(acc)\n",
    "print(np.mean(acc))\n",
    "\n",
    "clf_lr.fit(train,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.47079038  0.47241379  0.47241379  0.47241379  0.47404844]\n",
      "0.472416040045\n",
      "[ 0.83848797  0.85862069  0.86896552  0.84482759  0.8200692 ]\n",
      "0.846194193953\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=0.1, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=123, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Support Vector Machine Classifier\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "\n",
    "clf_svc = SVC(C=0.1,random_state=123)\n",
    "acc = cross_val_score(clf_svc,train,Y,scoring=\"accuracy\",cv=cv)\n",
    "print(acc)\n",
    "print(np.mean(acc))\n",
    "\n",
    "clf_svc.fit(train,Y)\n",
    "\n",
    "clf_linear_svc = LinearSVC(C=0.1,random_state=123)\n",
    "acc = cross_val_score(clf_linear_svc,train,Y,scoring=\"accuracy\",cv=cv)\n",
    "print(acc)\n",
    "print(np.mean(acc))\n",
    "\n",
    "clf_linear_svc.fit(train,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Random Forest Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.8556701   0.86896552  0.86551724  0.84827586  0.89273356]\n",
      "0.866232457559\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', alpha=0.01, batch_size='auto',\n",
       "       beta_1=0.9, beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(5, 10), learning_rate='adaptive',\n",
       "       learning_rate_init=0.001, max_iter=100, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=123,\n",
       "       shuffle=True, solver='lbfgs', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Neural Network Classifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "clf_NN = MLPClassifier(hidden_layer_sizes=(5,10),activation=\"logistic\",solver=\"lbfgs\",alpha=0.01,learning_rate=\"adaptive\",max_iter=100,shuffle=True,early_stopping=True,random_state=123)\n",
    "acc = cross_val_score(clf_NN,train,Y,scoring=\"accuracy\",cv=cv)\n",
    "print(acc)\n",
    "print(np.mean(acc))\n",
    "\n",
    "clf_NN.fit(train,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Clustering Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testData = []\n",
    "file_list = [f for f in listdir(testSnippetFolder) if not f.startswith('.')]\n",
    "\n",
    "for file in file_list:\n",
    "    filePath = testSnippetFolder + \"/\" + file\n",
    "    f = open(filePath,'r',encoding='utf-8')\n",
    "    data = f.readlines()\n",
    "    textSnippet = ''.join(data)\n",
    "    textSnippet = textSnippet.replace(\"\\n\",\"\")\n",
    "    for p in puncs:\n",
    "        if p in textSnippet:\n",
    "            textSnippet = textSnippet.replace(p,\"\")\n",
    "\n",
    "    testData.append(textSnippet)\n",
    "    \n",
    "#tfidf_test = TfidfVectorizer(input='content',encoding='utf-8',decode_error='ignore',strip_accents='ascii',ngram_range=(2,2),stop_words='english',max_features=None,norm='l1')\n",
    "test = tfidf.transform(testData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2625\n",
      "2625\n"
     ]
    }
   ],
   "source": [
    "#print(clf_lr.coef_)\n",
    "\n",
    "Y_test_lr = clf_lr.predict(test)\n",
    "\n",
    "Y_test_svc = clf_svc.predict(test)\n",
    "\n",
    "Y_test_linear_svc = clf_linear_svc.predict(test)\n",
    "\n",
    "Y_test_nn = clf_NN.predict(test)\n",
    "\n",
    "print(len(file_list))\n",
    "print(len(Y_test_lr))\n",
    "\n",
    "l = len(file_list)\n",
    "final_list = []\n",
    "for i in range(l):\n",
    "    final_list.append((file_list[i],Y_test_nn[i]))\n",
    "#print(final_list)  \n",
    "\n",
    "d = dict()\n",
    "for t in final_list:\n",
    "    #print(t)\n",
    "    #print(t[0],t[1])\n",
    "    if t[1] not in d.keys():\n",
    "        d[t[1]] = [t[0]]\n",
    "    else:\n",
    "        d[t[1]].append(t[0])\n",
    "\n",
    "for key, values in d.items():\n",
    "    with open(folderToSaveResultsFile + \"/\" + key + \".txt\",\"a\") as out_file:\n",
    "        for item in values :\n",
    "            out_file.write(item+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#find out the top features used by each classifier"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
